{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOStPJjAsdAwtlvZIbVDhAW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KPAryan/Chillers/blob/main/trasformer_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv1GTvGb_j1l",
        "outputId": "ab6d8d2d-1c2d-4677-bb6f-2b83a4b966fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy requests torch tiktoken matplotlib pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "dho_pJAN_xAv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 4  # How many batches per training step\n",
        "context_length = 16  # Length of the token chunk each batch\n",
        "d_model = 64  # The vector size of the token embeddings\n",
        "num_layers = 8  # Number of transformer blocks\n",
        "num_heads = 4  # Number of heads in Multi-head attention\n",
        "learning_rate = 1e-3  # 0.001\n",
        "dropout = 0.1 # Dropout rate\n",
        "max_iters = 5000  # Total of training iterations\n",
        "eval_interval = 50  # How often to evaluate the model\n",
        "eval_iters = 20  # How many iterations to average the loss over when evaluating the model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Instead of using the cpu, we'll use the GPU if it's available.\n",
        "\n",
        "TORCH_SEED = 1337\n",
        "torch.manual_seed(TORCH_SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e9BJzXuAAzv",
        "outputId": "499a9784-fd1e-4e64-e4e8-fe0386bd86b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a8d42e65ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download a sample txt file from https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt\n",
        "if not os.path.exists('sales_textbook.txt'):\n",
        "    url = 'https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt'\n",
        "    with open('sales_textbook.txt', 'w') as f:\n",
        "        f.write(requests.get(url).text)\n",
        "\n",
        "with open('sales_textbook.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "ZgE8aFv_ALrV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using TikToken to tokenize the source text\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "tokenized_text = encoding.encode(text) # size of tokenized source text is 77,919\n",
        "vocab_size = len(set(tokenized_text)) # size of vocabulary is 3,771\n",
        "max_token_value = max(tokenized_text)\n",
        "\n",
        "print(f\"Tokenized text size: {len(tokenized_text)}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"The maximum value in the tokenized text is: {max_token_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRkFg3ahAODj",
        "outputId": "0497f9d6-415e-4c69-dbc3-7a060a2ee362"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized text size: 77919\n",
            "Vocabulary size: 3771\n",
            "The maximum value in the tokenized text is: 100069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training batch\n",
        "data = train_data\n",
        "idxs = torch.randint(low=0, high=len(data) - context_length, size=(batch_size,))\n",
        "# Convert list slices to tensors before stacking\n",
        "x_batch = torch.stack([torch.tensor(data[idx:idx + context_length]) for idx in idxs])\n",
        "y_batch = torch.stack([torch.tensor(data[idx + 1:idx + context_length + 1]) for idx in idxs])\n",
        "print(x_batch.shape, x_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3C7wxsLAcZd",
        "outputId": "342d8aa7-c202-4aaa-ce10-a2266cdadb7e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16]) torch.Size([4, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Token Embedding look-up table\n",
        "token_embedding_lookup_table = nn.Embedding(max_token_value, d_model)\n",
        "\n",
        "# Get X and Y embedding\n",
        "x = token_embedding_lookup_table(x_batch.data)\n",
        "y = token_embedding_lookup_table(y_batch.data)"
      ],
      "metadata": {
        "id": "bEQZCafCAqH7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get x and y embedding\n",
        "x = token_embedding_lookup_table(x_batch.data) # [4, 16, 64] [batch_size, context_length, d_model]\n",
        "y = token_embedding_lookup_table(y_batch.data)"
      ],
      "metadata": {
        "id": "42IS8sZRAvgL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Position Encoding look-up table\n",
        "position_encoding_lookup_table = torch.zeros(context_length, d_model) # initial with zeros with shape (context_length, d_model)\n",
        "position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
        "# apply the sine & cosine\n",
        "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "position_encoding_lookup_table[:, 0::2] = torch.sin(position * div_term)\n",
        "position_encoding_lookup_table[:, 1::2] = torch.cos(position * div_term)\n",
        "position_encoding_lookup_table = position_encoding_lookup_table.unsqueeze(0).expand(batch_size, -1, -1) #add batch to the first dimension\n",
        "\n",
        "print(\"Position Encoding Look-up Table: \", position_encoding_lookup_table.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otsxmPV5AzKf",
        "outputId": "6016aa80-395e-4ad5-82f7-25b5ccaa5b90"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Position Encoding Look-up Table:  torch.Size([4, 16, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add positional encoding into the input embedding vector\n",
        "input_embedding_x = x + position_encoding_lookup_table # [4, 16, 64] [batch_size, context_length, d_model]\n",
        "input_embedding_y = y + position_encoding_lookup_table\n",
        "\n",
        "X = input_embedding_x\n",
        "\n",
        "x_plot = input_embedding_x[0].detach().cpu().numpy()\n",
        "print(\"Final Input Embedding of x: \\n\", pd.DataFrame(x_plot))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REO8wh7xA3zH",
        "outputId": "1b838b5a-1833-4577-f95c-7b8c520adfae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Input Embedding of x: \n",
            "           0         1         2         3         4         5         6   \\\n",
            "0  -0.299130  0.949924 -0.631772  0.939232 -2.082235  1.398602 -0.240277   \n",
            "1   0.758386  0.999339  0.238804  2.130460 -0.736705  1.686566  0.824980   \n",
            "2  -0.922071 -2.250749 -0.556319  1.222940  2.109022 -0.400113 -0.138789   \n",
            "3   0.306074 -1.573041  2.231532  0.032769  1.850678 -0.120396 -0.011787   \n",
            "4  -0.954794 -0.976841  0.139319 -0.865764 -0.193850 -1.655313  1.521742   \n",
            "5   0.679198  0.506228  0.353623 -0.863025 -2.570290 -1.323900  0.371723   \n",
            "6  -0.323705  2.691280 -2.467007  0.717207  1.387555 -2.344249  0.049187   \n",
            "7   0.476250  1.532691 -0.440719  0.884680 -0.049779 -0.415280 -0.095141   \n",
            "8  -1.377338 -0.241152  0.619716  0.535313 -2.747910 -0.546994 -1.181236   \n",
            "9  -0.405825  0.559437  1.838797  0.457612 -2.470442  1.303259 -0.322104   \n",
            "10 -0.724732 -1.171013 -0.256460 -1.604869 -1.592075  0.130025  0.570488   \n",
            "11 -0.449413 -0.703517  0.179154  0.373091 -0.182106  1.931234 -0.799360   \n",
            "12  0.341896  1.383503 -0.627911 -1.276247  1.170495  1.140588 -1.435867   \n",
            "13  0.524299  0.225087 -1.034895  0.773311  2.538593  0.801516 -2.413383   \n",
            "14  1.239190 -0.703227 -0.857716 -0.738189  2.185374  1.034195  0.174827   \n",
            "15  2.108387 -0.318655 -0.287144  0.649548  0.771553 -0.321154  1.457626   \n",
            "\n",
            "          7         8         9   ...        54        55        56        57  \\\n",
            "0   1.460907  0.438821  0.568639  ...  1.452847  1.467532  1.984371  0.791459   \n",
            "1   0.610698 -0.504885  1.616139  ...  0.448665 -2.071404  0.672218  2.796822   \n",
            "2  -1.933866 -1.671411  1.150713  ... -0.686295 -0.096573 -2.178207  0.451573   \n",
            "3  -0.635150  0.029269  1.307228  ... -0.607196  2.235353 -0.330132  0.043395   \n",
            "4  -0.318029 -0.214993  0.665824  ... -0.545532 -0.345191  0.142483  1.507527   \n",
            "5  -1.104829  0.616293  0.783596  ...  1.080138 -0.161667 -1.531240  1.557224   \n",
            "6   0.084035  1.446481  0.074499  ...  0.716758  1.740306 -1.434396  0.254191   \n",
            "7  -1.715202 -0.558720 -1.187362  ...  0.746224  0.008394 -0.101539 -0.490031   \n",
            "8  -3.191279 -0.665482 -1.169012  ... -0.079880  3.122454 -2.058005  1.065758   \n",
            "9   0.723305 -1.070696 -1.178378  ... -0.280317  1.782857 -1.054796  1.835647   \n",
            "10 -1.642119  0.801727 -1.764521  ... -0.340848  3.639359 -0.644349  3.063182   \n",
            "11  0.825862 -0.736930  0.177177  ... -0.280090  1.006504  1.357095  1.059691   \n",
            "12  1.224846 -0.545266 -1.121737  ... -1.307938  2.227912 -1.178179  1.575556   \n",
            "13 -1.562514  0.118980  0.325631  ...  0.397413  0.479963  0.217271  2.055336   \n",
            "14  1.099753 -1.613777 -1.243054  ...  0.359625  1.258582  1.067245  1.663585   \n",
            "15  0.268369 -0.826254 -0.871297  ...  1.625291  0.643504 -1.032252  1.950458   \n",
            "\n",
            "          58        59        60        61        62        63  \n",
            "0   0.456751  0.789545 -2.045897  1.099113  0.166035  3.139895  \n",
            "1   0.437217  2.284778 -1.708361 -0.152342  0.699953  0.343699  \n",
            "2  -0.500310  1.891739  0.073773  0.102590 -0.138560  0.840906  \n",
            "3   1.379010  1.228062 -0.214476  0.090425 -0.173942 -0.830836  \n",
            "4   0.027854 -0.008453 -1.213767 -0.094705  0.886935  1.103025  \n",
            "5  -0.348979  1.005130 -0.339587  0.136550 -0.597760  1.012034  \n",
            "6  -0.240991  0.542596  0.039506  2.375268  0.122078  1.598531  \n",
            "7  -0.253113  1.570766 -0.642553  0.589649 -0.934843 -1.100760  \n",
            "8   0.787592  2.288829  0.152384  1.582828 -0.278190  1.500612  \n",
            "9  -2.389836  2.359888 -0.895393  2.478489  1.150637  2.515316  \n",
            "10  1.745668  1.311434 -0.703046  0.981743  0.178330  0.443150  \n",
            "11  1.175118  1.527424 -0.844068  1.202420 -0.927549  0.908416  \n",
            "12  0.974123  1.321630 -0.581522 -1.142979 -0.997342  2.858498  \n",
            "13  0.841140  0.250887  1.338835  1.003394  1.427358  1.072308  \n",
            "14  0.892081  1.761903  0.728198  1.667248 -1.340948  0.224300  \n",
            "15  0.948557  2.855112 -0.750874  2.738219  0.865055  2.720319  \n",
            "\n",
            "[16 rows x 64 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Query, Key, Value for Multi-head Attention\n",
        "\n",
        "query = key = value = X # [4, 16, 64] [batch_size, context_length, d_model]\n",
        "\n",
        "# Define Query, Key, Value weight matrices\n",
        "Wq = nn.Linear(d_model, d_model)\n",
        "Wk = nn.Linear(d_model, d_model)\n",
        "Wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "Q = Wq(query) #[4, 16, 64]\n",
        "Q = Q.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]\n",
        "\n",
        "K = Wk(key) #[4, 16, 64]\n",
        "K = K.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]\n",
        "\n",
        "V = Wv(value) #[4, 16, 64]\n",
        "V = V.view(batch_size, -1, num_heads, d_model // num_heads)  #[4, 16, 4, 16]"
      ],
      "metadata": {
        "id": "0cfON3cuA9IC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose q,k,v from [batch_size, context_length, num_heads, head_size] to [batch_size, num_heads, context_length, head_size]\n",
        "# The reason is that treat each batch with \"num_heads\" as its first dimension.\n",
        "Q = Q.transpose(1, 2) # [4, 4, 16, 16]\n",
        "K = K.transpose(1, 2) # [4, 4, 16, 16]\n",
        "V = V.transpose(1, 2) # [4, 4, 16, 16]"
      ],
      "metadata": {
        "id": "nunp8x-BBD1Y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_score = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model // num_heads) # [4, 4, 16, 16] #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
        "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsDn2OwPBH8A",
        "outputId": "c4da6fea-a93e-40f7-e4c8-393427bc514e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.021498  0.759700  1.114897  0.861203  0.996177  0.087225  0.020397   \n",
            "1  -0.186354  0.423521  0.930679  0.397063  0.689583 -0.023029 -1.295206   \n",
            "2  -0.116987 -0.521399 -0.102028 -0.048670 -0.094187  0.211903  0.785083   \n",
            "3   0.677378  0.094782  0.187630  1.087047  0.387743  0.641712  1.120173   \n",
            "4   0.631352 -0.280073 -0.932875 -0.581271 -0.206257  0.127698 -0.102794   \n",
            "5  -0.327146  0.068143  0.148191 -0.391794  0.132223  0.017728  0.268415   \n",
            "6  -0.444339  0.258554  0.455449 -0.077464  0.134794 -0.053327  0.101352   \n",
            "7  -0.245799  0.580802  0.932249  0.135226  0.357541 -0.006203  0.136920   \n",
            "8  -0.303093  0.018890  0.015307  0.294602  0.369652 -0.219553 -0.192504   \n",
            "9  -0.136668  0.443592  0.470723 -0.184153  0.221327 -0.058326 -0.335395   \n",
            "10  0.694354  0.946938  0.422884  0.789917  0.990797 -0.172340  0.704143   \n",
            "11  0.352159  0.977470  1.040248  1.317057  1.359986  0.120361  0.799884   \n",
            "12 -0.238808  0.104332  0.052890 -0.593303 -0.794684 -0.462218  0.311895   \n",
            "13 -0.517919  0.067904  0.636224  0.144879  0.018605  0.213238  0.280446   \n",
            "14 -0.085574 -0.059928  0.476232 -0.008881 -0.143431  0.318674  0.256556   \n",
            "15 -0.184219 -0.247370  0.608031  0.304992  0.352951  0.453829  0.285533   \n",
            "\n",
            "          7         8         9         10        11        12        13  \\\n",
            "0   0.353933 -0.263620 -0.481791 -1.015287 -0.027799 -0.770300  1.028327   \n",
            "1   0.035366 -0.226877 -1.094293 -0.077335  0.149487 -1.291372 -0.246792   \n",
            "2   0.184757 -0.163499  0.059236  0.515760  0.117891  0.133373  0.033434   \n",
            "3   0.259390 -0.339914  0.120134 -0.394649 -0.246437 -0.579486  0.994677   \n",
            "4  -0.442630  0.287699 -0.108314  0.540231  0.316796 -0.293156 -0.427342   \n",
            "5   0.222270  0.331641 -0.015495 -0.366638  0.014746 -0.224710  0.378336   \n",
            "6   0.162352  0.333664  0.069486 -0.037877 -0.015095 -0.301473  0.237264   \n",
            "7   0.361665  0.171941 -0.240028 -0.460423 -0.134655  0.122417  0.401689   \n",
            "8  -0.606256 -0.548382 -0.053605 -0.832274 -0.339201 -0.297512  0.289678   \n",
            "9   0.530910  0.210424  0.113577 -0.280486  0.202264  0.089873  0.381280   \n",
            "10 -0.240142 -0.371990 -0.299596 -0.832197 -0.359611 -0.637496  0.765209   \n",
            "11  0.680050  0.324915 -0.482536 -0.499427  0.151246 -0.254691  1.035710   \n",
            "12  0.081620 -0.338989 -0.082385 -0.711630 -0.425134  0.018855  0.009447   \n",
            "13  0.551979  0.049975 -0.170539  0.008342  0.121066  0.532295  0.402487   \n",
            "14  0.872133  0.463599  0.522785  0.704850  0.512636  0.430275  0.164187   \n",
            "15  0.616212  0.059369  0.419577  0.663161  0.387546  0.333471  0.249560   \n",
            "\n",
            "          14        15  \n",
            "0   0.241509  0.527406  \n",
            "1  -0.591006  0.052637  \n",
            "2   0.032123  0.521154  \n",
            "3  -0.021960  0.356922  \n",
            "4  -0.393042 -0.474839  \n",
            "5   0.081683  0.197241  \n",
            "6   0.111261  0.401928  \n",
            "7   0.278085  0.003031  \n",
            "8   0.017494  0.183737  \n",
            "9   0.191927  0.236189  \n",
            "10  0.152228  0.264614  \n",
            "11  0.668065  0.268745  \n",
            "12 -0.214531 -0.060118  \n",
            "13  0.094902  0.129446  \n",
            "14  0.546852  0.545444  \n",
            "15  0.546221  0.853163  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Mask to attention scores\n",
        "attention_score = attention_score.masked_fill(torch.triu(torch.ones(attention_score.shape[-2:]), diagonal=1).bool(), float('-inf')) #[4, 4, 16, 16] [batch_size, num_heads, context_length, context_length]\n",
        "print(pd.DataFrame(attention_score[0][0].detach().cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAR204PsBLa_",
        "outputId": "c83c11d1-0482-46c3-80a2-5b5a1f7b1be4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0         1         2         3         4         5         6   \\\n",
            "0   0.021498      -inf      -inf      -inf      -inf      -inf      -inf   \n",
            "1  -0.186354  0.423521      -inf      -inf      -inf      -inf      -inf   \n",
            "2  -0.116987 -0.521399 -0.102028      -inf      -inf      -inf      -inf   \n",
            "3   0.677378  0.094782  0.187630  1.087047      -inf      -inf      -inf   \n",
            "4   0.631352 -0.280073 -0.932875 -0.581271 -0.206257      -inf      -inf   \n",
            "5  -0.327146  0.068143  0.148191 -0.391794  0.132223  0.017728      -inf   \n",
            "6  -0.444339  0.258554  0.455449 -0.077464  0.134794 -0.053327  0.101352   \n",
            "7  -0.245799  0.580802  0.932249  0.135226  0.357541 -0.006203  0.136920   \n",
            "8  -0.303093  0.018890  0.015307  0.294602  0.369652 -0.219553 -0.192504   \n",
            "9  -0.136668  0.443592  0.470723 -0.184153  0.221327 -0.058326 -0.335395   \n",
            "10  0.694354  0.946938  0.422884  0.789917  0.990797 -0.172340  0.704143   \n",
            "11  0.352159  0.977470  1.040248  1.317057  1.359986  0.120361  0.799884   \n",
            "12 -0.238808  0.104332  0.052890 -0.593303 -0.794684 -0.462218  0.311895   \n",
            "13 -0.517919  0.067904  0.636224  0.144879  0.018605  0.213238  0.280446   \n",
            "14 -0.085574 -0.059928  0.476232 -0.008881 -0.143431  0.318674  0.256556   \n",
            "15 -0.184219 -0.247370  0.608031  0.304992  0.352951  0.453829  0.285533   \n",
            "\n",
            "          7         8         9         10        11        12        13  \\\n",
            "0       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
            "1       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
            "2       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
            "3       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
            "4       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
            "5       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
            "6       -inf      -inf      -inf      -inf      -inf      -inf      -inf   \n",
            "7   0.361665      -inf      -inf      -inf      -inf      -inf      -inf   \n",
            "8  -0.606256 -0.548382      -inf      -inf      -inf      -inf      -inf   \n",
            "9   0.530910  0.210424  0.113577      -inf      -inf      -inf      -inf   \n",
            "10 -0.240142 -0.371990 -0.299596 -0.832197      -inf      -inf      -inf   \n",
            "11  0.680050  0.324915 -0.482536 -0.499427  0.151246      -inf      -inf   \n",
            "12  0.081620 -0.338989 -0.082385 -0.711630 -0.425134  0.018855      -inf   \n",
            "13  0.551979  0.049975 -0.170539  0.008342  0.121066  0.532295  0.402487   \n",
            "14  0.872133  0.463599  0.522785  0.704850  0.512636  0.430275  0.164187   \n",
            "15  0.616212  0.059369  0.419577  0.663161  0.387546  0.333471  0.249560   \n",
            "\n",
            "          14        15  \n",
            "0       -inf      -inf  \n",
            "1       -inf      -inf  \n",
            "2       -inf      -inf  \n",
            "3       -inf      -inf  \n",
            "4       -inf      -inf  \n",
            "5       -inf      -inf  \n",
            "6       -inf      -inf  \n",
            "7       -inf      -inf  \n",
            "8       -inf      -inf  \n",
            "9       -inf      -inf  \n",
            "10      -inf      -inf  \n",
            "11      -inf      -inf  \n",
            "12      -inf      -inf  \n",
            "13      -inf      -inf  \n",
            "14  0.546852      -inf  \n",
            "15  0.546221  0.853163  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import math\n",
        "\n",
        "# Assuming attention_score is your 4D tensor\n",
        "\n",
        "# Select the first batch and the first head\n",
        "batch_index = 0\n",
        "head_index = 0\n",
        "attention_score_2d = attention_score[batch_index, head_index].detach().cpu().numpy()\n",
        "\n",
        "# Create the DataFrame\n",
        "df = pd.DataFrame(attention_score_2d)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pujuwue8BaE7",
        "outputId": "9abbecc4-aab2-410c-aebb-e63f69ef7d82"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0         1         2         3         4         5         6   \\\n",
            "0   1.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1   0.352088  0.647912  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "2   0.372795  0.248792  0.378413  0.000000  0.000000  0.000000  0.000000   \n",
            "3   0.271920  0.151853  0.166628  0.409599  0.000000  0.000000  0.000000   \n",
            "4   0.427102  0.171674  0.089371  0.127027  0.184826  0.000000  0.000000   \n",
            "5   0.124600  0.185008  0.200427  0.116800  0.197252  0.175912  0.000000   \n",
            "6   0.083958  0.169561  0.206461  0.121170  0.149823  0.124131  0.144895   \n",
            "7   0.069452  0.158736  0.225584  0.101663  0.126974  0.088256  0.101836   \n",
            "8   0.088831  0.122574  0.122136  0.161488  0.174074  0.096571  0.099218   \n",
            "9   0.073767  0.131784  0.135409  0.070346  0.105520  0.079778  0.060472   \n",
            "10  0.121218  0.156050  0.092400  0.133374  0.163047  0.050953  0.122411   \n",
            "11  0.060166  0.112440  0.119725  0.157907  0.164834  0.047718  0.094145   \n",
            "12  0.072694  0.102453  0.097315  0.050997  0.041695  0.058140  0.126086   \n",
            "13  0.034521  0.062015  0.109476  0.066978  0.059032  0.071716  0.076702   \n",
            "14  0.042108  0.043202  0.073850  0.045464  0.039741  0.063085  0.059286   \n",
            "15  0.035042  0.032898  0.077386  0.057155  0.059963  0.066327  0.056053   \n",
            "\n",
            "          7         8         9         10        11        12        13  \\\n",
            "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "3   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "7   0.127499  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "8   0.065600  0.069508  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "9   0.143809  0.104376  0.094741  0.000000  0.000000  0.000000  0.000000   \n",
            "10  0.047613  0.041731  0.044864  0.026339  0.000000  0.000000  0.000000   \n",
            "11  0.083513  0.058549  0.026113  0.025675  0.049215  0.000000  0.000000   \n",
            "12  0.100152  0.065764  0.085003  0.045306  0.060336  0.094059  0.000000   \n",
            "13  0.100631  0.060913  0.048859  0.058429  0.065401  0.098669  0.086658   \n",
            "14  0.109721  0.072923  0.077370  0.092820  0.076588  0.070533  0.054055   \n",
            "15  0.078021  0.044707  0.064094  0.081772  0.062073  0.058806  0.054073   \n",
            "\n",
            "          14        15  \n",
            "0   0.000000  0.000000  \n",
            "1   0.000000  0.000000  \n",
            "2   0.000000  0.000000  \n",
            "3   0.000000  0.000000  \n",
            "4   0.000000  0.000000  \n",
            "5   0.000000  0.000000  \n",
            "6   0.000000  0.000000  \n",
            "7   0.000000  0.000000  \n",
            "8   0.000000  0.000000  \n",
            "9   0.000000  0.000000  \n",
            "10  0.000000  0.000000  \n",
            "11  0.000000  0.000000  \n",
            "12  0.000000  0.000000  \n",
            "13  0.000000  0.000000  \n",
            "14  0.079254  0.000000  \n",
            "15  0.072747  0.098883  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the V attention output\n",
        "attention_output = torch.matmul(attention_score, V) # [4, 4, 16, 16] [batch_size, num_heads, context_length, head_size]\n",
        "print(attention_output.shape) # Now this line should work correctly."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkkXhejIBqc7",
        "outputId": "2dad6974-fc28-431f-842b-5e45e3dcad11"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 4, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = A.transpose(1, 2) # [4, 16, 4, 16] [batch_size, context_length, num_heads, head_size]\n",
        "A = A.reshape(batch_size, -1, d_model) # [4, 16, 64] [batch_size, context_length, d_model]"
      ],
      "metadata": {
        "id": "FXp8Em_RCARF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the output weight matrix\n",
        "Wo = nn.Linear(d_model, d_model)\n",
        "output = Wo(A) # [4, 16, 64] [batch_size, context_length, d_model]\n",
        "\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw6JgJkYCB2d",
        "outputId": "bd99319f-191f-4ad3-d08b-3c8b18602ab5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 16, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add residual connection\n",
        "output = output + X\n",
        "\n",
        "# Add Layer Normalization\n",
        "layer_norm = nn.LayerNorm(d_model)\n",
        "output = layer_norm(output)"
      ],
      "metadata": {
        "id": "Uh-vjdTFCLYB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Feed Forward Network\n",
        "output = nn.Linear(d_model, d_model * 4)(output)\n",
        "output = nn.ReLU()(output)\n",
        "output = nn.Linear(d_model * 4, d_model)(output)\n",
        "output = torch.dropout(output, p=dropout, train=True)"
      ],
      "metadata": {
        "id": "GL5BRv5cCPb6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add residual connection\n",
        "output = output + X\n",
        "# Add Layer Normalization\n",
        "layer_norm = nn.LayerNorm(d_model)\n",
        "output = layer_norm(output)"
      ],
      "metadata": {
        "id": "P8uxN2cBCXPb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = nn.Linear(d_model, max_token_value)(output)\n",
        "print(pd.DataFrame(logits[0].detach().cpu().numpy()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAl-KlabCZEK",
        "outputId": "2d63f3f1-fece-4020-98e6-983a365ddf53"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0         1         2         3         4         5         6       \\\n",
            "0   0.667483 -0.389633 -0.851936 -0.394342 -0.998247  0.262954  0.480094   \n",
            "1  -0.038112  0.322328 -0.031820 -0.584922 -0.112201 -0.738884  0.440067   \n",
            "2  -0.628515  0.205482 -0.180367 -0.370944  0.412005  0.792747 -0.194784   \n",
            "3  -1.048590  0.793600 -0.683289 -0.619398 -0.317157  0.628633 -0.024611   \n",
            "4  -0.716813 -0.497194 -0.200344 -1.548010 -0.219922  0.368706 -0.773210   \n",
            "5  -0.378771  1.055804 -0.537877  0.158270 -0.003828 -0.635562  0.394238   \n",
            "6  -0.310792 -0.171071  0.622006 -0.609642  0.586624  0.604495  0.867905   \n",
            "7  -0.922160  0.500379 -0.034140  0.611128 -0.062309  0.604757  0.336614   \n",
            "8   0.318265  0.177395  0.507006 -0.731018 -0.162960  0.023586  0.724885   \n",
            "9   1.179621  0.450486 -0.439893  0.052057  0.397160 -1.076628  0.009521   \n",
            "10  0.329482 -0.249196 -0.487763  0.146532  0.203778  0.126987 -0.013209   \n",
            "11  0.491254  0.254098 -0.805063  0.847444 -0.868649 -0.204639  0.187366   \n",
            "12  0.908632 -0.443110  0.164215 -0.860427 -0.831706 -0.385907 -0.045611   \n",
            "13 -0.267631 -0.724435 -0.408752  0.460729 -0.677400  0.245174  0.588925   \n",
            "14 -0.226249  0.275237 -0.203830  0.269060 -0.362902  0.613476  0.069186   \n",
            "15  0.445209  0.237637  0.394769  0.191370  0.002953  0.345110  0.509143   \n",
            "\n",
            "      7         8         9       ...    100059    100060    100061    100062  \\\n",
            "0  -0.170555  0.321491 -0.220245  ...  0.564134 -0.215222  0.511216 -0.345695   \n",
            "1  -0.064511 -0.477147  0.017552  ... -0.518621 -0.709125 -0.062348  0.385515   \n",
            "2   0.090611  0.116007 -0.432469  ... -0.815883 -0.021172  0.105399 -0.184825   \n",
            "3  -0.684836  0.315627 -0.726044  ...  0.343552 -0.406947  0.113339  0.518061   \n",
            "4   0.131509  1.002828 -0.384169  ... -0.272205 -0.356717 -0.617441  0.507051   \n",
            "5   0.205076 -0.376159 -0.327241  ...  0.802938 -0.170070 -0.385224  0.149915   \n",
            "6   0.189337 -0.016440 -0.140744  ...  1.027234  0.717433  0.220867  0.117609   \n",
            "7  -0.430542 -0.866047 -1.096003  ... -0.119494  0.724068 -0.010441 -0.006981   \n",
            "8   1.001006 -0.591480 -0.908987  ... -0.415700 -0.490938  0.141666  0.299761   \n",
            "9  -0.407299 -0.337472 -0.405212  ...  0.243390 -0.221569  0.333176 -0.685081   \n",
            "10  0.003749 -0.731550 -0.305711  ...  0.174980 -0.875414  0.528700  0.273198   \n",
            "11 -0.176603  0.268296 -1.479937  ...  0.183884 -0.223598  0.664912  0.249806   \n",
            "12 -0.186296 -0.316574 -0.322434  ... -0.309264 -0.024482  0.413519  0.471669   \n",
            "13 -0.478098 -0.365851 -1.100504  ...  0.009546 -0.100925  0.359181 -0.584053   \n",
            "14 -0.556882 -0.456754  0.220161  ...  0.158783  0.210422  0.217538 -0.163716   \n",
            "15 -0.298419 -0.693817  0.393190  ... -0.114032  0.024332  0.364460 -0.628126   \n",
            "\n",
            "      100063    100064    100065    100066    100067    100068  \n",
            "0   0.280026 -0.349468 -1.221684  0.165527 -0.370290 -0.554029  \n",
            "1   0.757354  0.018531  0.216118  0.658736  0.896402  0.008098  \n",
            "2   0.522658  0.539416 -0.011119 -0.354371  0.286689  0.178755  \n",
            "3   0.095571  0.447914  0.503538 -0.180665 -1.383265  0.025491  \n",
            "4   0.071807 -0.294688  0.486347 -0.558503  0.035770  0.591002  \n",
            "5  -0.481235  0.759102 -0.394108  0.041804 -0.036551  0.057988  \n",
            "6  -0.191414  0.218748 -0.374619  0.706337 -0.422270  0.440738  \n",
            "7   0.553941  0.703121 -0.167306  1.422663  0.062663 -0.328725  \n",
            "8   0.190456  0.520707  0.083594 -0.300857 -1.272671  0.048819  \n",
            "9  -0.171625  0.068226  0.191056 -0.876465  0.260361  0.481487  \n",
            "10  0.839494  0.547030  0.100529 -0.164694 -0.530678 -0.605713  \n",
            "11 -0.750480  0.348481  0.068968 -0.156382 -0.645810  0.348471  \n",
            "12  0.029170  0.117407  0.952073  0.557498 -0.423473  0.781243  \n",
            "13 -0.187492  0.196010 -0.530906 -0.222555  0.607256  0.519245  \n",
            "14 -0.704219  0.359411  0.344767  0.353608 -0.194872  0.841876  \n",
            "15  0.173693  0.330141  0.751014 -0.535763  0.894746  0.147730  \n",
            "\n",
            "[16 rows x 100069 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "what we get here is a huge matrix with shape [16, 100069] which is the probabilities of each token in the whole vocabulary."
      ],
      "metadata": {
        "id": "P-YtVwDkCu2t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rShlaiVFCoAN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}